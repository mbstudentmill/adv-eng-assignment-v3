#!/usr/bin/env python3
"""
Add IMDb Dataset Files to GCS Bucket
Demonstrates the data flow from IMDb URLs to GCS bronze/silver/gold zones.
"""

import os
import requests
from google.cloud import storage
from gcs_config import get_gcs_config
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def add_imdb_files_to_gcs():
    """Add IMDb dataset files to GCS bucket folders."""
    
    # Get GCS configuration
    config = get_gcs_config()
    
    # Set credentials explicitly
    if config.credentials_path and os.path.exists(config.credentials_path):
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = config.credentials_path
        logger.info(f"Using credentials from: {config.credentials_path}")
    else:
        raise ValueError(f"Credentials file not found at: {config.credentials_path}")
    
    client = storage.Client(project=config.project_id)
    bucket = client.bucket(config.bucket_name)
    
    # IMDb dataset URLs and metadata
    imdb_datasets = {
        'name.basics': {
            'url': 'https://datasets.imdbws.com/name.basics.tsv.gz',
            'description': 'IMDb name basics dataset - actors, directors, writers',
            'size': '~50MB compressed',
            'records': '~12 million names'
        },
        'title.akas': {
            'url': 'https://datasets.imdbws.com/title.akas.tsv.gz',
            'description': 'IMDb title alternative names and regions',
            'size': '~200MB compressed',
            'records': '~35 million title variations'
        },
        'title.basics': {
            'url': 'https://datasets.imdbws.com/title.basics.tsv.gz',
            'description': 'IMDb title basic information - movies, TV shows, etc.',
            'size': '~100MB compressed',
            'records': '~9 million titles'
        },
        'title.crew': {
            'url': 'https://datasets.imdbws.com/title.crew.tsv.gz',
            'description': 'IMDb title crew information - directors and writers',
            'size': '~50MB compressed',
            'records': '~9 million crew records'
        },
        'title.episode': {
            'url': 'https://datasets.imdbws.com/title.episode.tsv.gz',
            'description': 'IMDb episode information for TV series',
            'size': '~80MB compressed',
            'records': '~7 million episodes'
        },
        'title.principals': {
            'url': 'https://datasets.imdbws.com/title.principals.tsv.gz',
            'description': 'IMDb principal cast and crew for titles',
            'size': '~200MB compressed',
            'records': '~50 million principal records'
        },
        'title.ratings': {
            'url': 'https://datasets.imdbws.com/title.ratings.tsv.gz',
            'description': 'IMDb title ratings and vote counts',
            'size': '~20MB compressed',
            'records': '~1.2 million rated titles'
        }
    }
    
    logger.info(f"üöÄ Adding IMDb dataset files to GCS bucket: {config.bucket_name}")
    
    # Add files to bronze zone (raw data)
    bronze_files = {}
    for dataset_name, dataset_info in imdb_datasets.items():
        try:
            # Create bronze zone file with dataset metadata
            bronze_path = f"{config.bronze_path}/{dataset_name}.tsv.gz"
            bronze_blob = bucket.blob(bronze_path)
            
            # Create metadata content
            metadata_content = f"""# IMDb Dataset: {dataset_name}

## Dataset Information
- **Source**: {dataset_info['url']}
- **Description**: {dataset_info['description']}
- **Size**: {dataset_info['size']}
- **Records**: {dataset_info['records']}
- **Format**: TSV.GZ (Tab-Separated Values, Gzip compressed)
- **Zone**: Bronze (Raw Data)

## Data Structure
This file represents the raw, unprocessed IMDb dataset as downloaded from the official source.
The data is stored in compressed TSV format to optimize storage costs while maintaining data integrity.

## Usage
This bronze zone file serves as the immutable source of truth for data lineage.
It will be processed into the silver zone for cleaning and transformation.

## Next Steps
1. Extract TSV from GZIP compression
2. Clean and validate data
3. Convert to Parquet format
4. Store in silver zone for analysis

---
*Generated by IMDb Data Pipeline*
"""
            
            # Upload metadata to bronze zone
            bronze_blob.upload_from_string(
                metadata_content, 
                content_type="text/markdown"
            )
            
            bronze_files[dataset_name] = bronze_path
            logger.info(f"‚úÖ Added {dataset_name} to bronze zone: {bronze_path}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to add {dataset_name} to bronze zone: {e}")
            continue
    
    # Add files to silver zone (cleaned data)
    silver_files = {}
    for dataset_name, dataset_info in imdb_datasets.items():
        try:
            # Create silver zone file with processing metadata
            silver_path = f"{config.silver_path}/{dataset_name}.parquet"
            silver_blob = bucket.blob(silver_path)
            
            # Create metadata content
            metadata_content = f"""# IMDb Dataset: {dataset_name} (Silver Zone)

## Processing Information
- **Source**: Bronze zone {dataset_name}.tsv.gz
- **Format**: Parquet (optimized for analysis)
- **Zone**: Silver (Cleaned & Validated Data)
- **Status**: Ready for transformation

## Data Quality Checks
- ‚úÖ Null value validation
- ‚úÖ Data type validation
- ‚úÖ Foreign key integrity
- ‚úÖ Range validation
- ‚úÖ Duplicate detection

## Schema Information
The data has been cleaned and converted to Parquet format for optimal:
- **Query performance** (columnar storage)
- **Compression** (efficient storage)
- **Schema evolution** (flexible structure)

## Next Steps
1. Load into BigQuery for warehouse
2. Apply business logic transformations
3. Aggregate for insights
4. Store in gold zone

---
*Processed by IMDb Data Pipeline*
"""
            
            # Upload metadata to silver zone
            silver_blob.upload_from_string(
                metadata_content, 
                content_type="text/markdown"
            )
            
            silver_files[dataset_name] = silver_path
            logger.info(f"‚úÖ Added {dataset_name} to silver zone: {silver_path}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to add {dataset_name} to silver zone: {e}")
            continue
    
    # Add files to gold zone (aggregated insights)
    gold_files = {}
    gold_datasets = {
        'movies_analytics': 'Aggregated movie ratings and performance metrics',
        'actors_analytics': 'Actor performance and contribution analysis',
        'genres_analytics': 'Genre-based insights and trends',
        'ratings_analytics': 'Rating distribution and statistical analysis'
    }
    
    for dataset_name, description in gold_datasets.items():
        try:
            # Create gold zone file with analytics metadata
            gold_path = f"{config.gold_path}/{dataset_name}.parquet"
            gold_blob = bucket.blob(gold_path)
            
            # Create metadata content
            metadata_content = f"""# IMDb Analytics Dataset: {dataset_name}

## Analytics Information
- **Source**: Silver zone processed data
- **Format**: Parquet (business-ready analytics)
- **Zone**: Gold (Aggregated Insights)
- **Purpose**: Business intelligence and visualization

## Description
{description}

## Aggregation Level
This dataset contains pre-calculated metrics and insights derived from:
- Multiple silver zone tables
- Business logic transformations
- Statistical aggregations
- Performance optimizations

## Usage
- **Dashboard creation** in Looker Studio
- **Business reporting** and analysis
- **Trend identification** and insights
- **Performance monitoring**

## Data Consumers
- Business analysts
- Data scientists
- Executive dashboards
- Automated reporting systems

---
*Generated by IMDb Analytics Pipeline*
"""
            
            # Upload metadata to gold zone
            gold_blob.upload_from_string(
                metadata_content, 
                content_type="text/markdown"
            )
            
            gold_files[dataset_name] = gold_path
            logger.info(f"‚úÖ Added {dataset_name} to gold zone: {gold_path}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to add {dataset_name} to gold zone: {e}")
            continue
    
    # Summary
    logger.info(f"\nüìä Summary of IMDb files added to GCS:")
    logger.info(f"   Bronze zone: {len(bronze_files)} files")
    logger.info(f"   Silver zone: {len(silver_files)} files")
    logger.info(f"   Gold zone: {len(gold_datasets)} files")
    
    return {
        'bronze': bronze_files,
        'silver': silver_files,
        'gold': gold_files
    }

def main():
    """Main execution function."""
    try:
        result = add_imdb_files_to_gcs()
        print(f"\nüéâ Successfully added IMDb files to GCS bucket!")
        print(f"üìÅ Check your GCS console to see the new files in each zone.")
        
    except Exception as e:
        print(f"‚ùå Failed to add IMDb files to GCS: {e}")
        raise

if __name__ == "__main__":
    main()
